"""
Practical Use Cases with Vectorized Operations

This guide shows real-world scenarios where vectorized operations shine.
"""

import numpy as np
import pandas as pd
from collections import Counter
import sys
sys.path.append('.')
from indicator_importance import SnorkelIndicatorImportance

print("=" * 80)
print("PRACTICAL USE CASES - Vectorized Operations")
print("=" * 80)

# Setup: Create realistic dataset
np.random.seed(42)
n_samples = 5000
n_indicators = 50

indicator_matrix = np.random.choice([-1, 0, 1], size=(n_samples, n_indicators), p=[0.2, 0.5, 0.3])
snorkel_weights = np.random.randn(n_indicators) * 0.5
indicator_names = [f"LF_{i}_{np.random.choice(['accuracy', 'coverage', 'precision'])}" 
                   for i in range(n_indicators)]

calc = SnorkelIndicatorImportance(
    indicator_matrix=indicator_matrix,
    snorkel_weights=snorkel_weights,
    indicator_names=indicator_names
)

print(f"\nDataset: {n_samples:,} samples, {n_indicators} indicators")

# ============================================================================
# USE CASE 1: Production Model Monitoring
# ============================================================================
print("\n" + "=" * 80)
print("USE CASE 1: Daily Model Monitoring")
print("=" * 80)
print("""
Scenario: You have a production model making 10k predictions daily.
You want to monitor which labeling functions are driving decisions.
""")

import time
start = time.time()

# Get importance for all predictions
all_importance = calc.get_all_importance_scores(normalize=True)

# Aggregate statistics
mean_importance = np.mean(np.abs(all_importance), axis=0)
std_importance = np.std(np.abs(all_importance), axis=0)

# Identify most important LFs
top_lfs = np.argsort(mean_importance)[-10:][::-1]

elapsed = time.time() - start

print(f"\n‚úì Analyzed all {n_samples:,} predictions in {elapsed:.4f} seconds")
print(f"\nTop 10 Most Important LFs (averaged across all predictions):")
print(f"{'Rank':<6} {'LF Name':<35} {'Avg Importance':<15} {'Std Dev':<10}")
print("-" * 70)
for rank, idx in enumerate(top_lfs, 1):
    print(f"{rank:<6} {indicator_names[idx]:<35} {mean_importance[idx]:>13.4f}  {std_importance[idx]:>8.4f}")

print(f"\nüí° Insight: Monitor these top LFs closely - they drive most decisions")

# ============================================================================
# USE CASE 2: Finding Problematic Predictions
# ============================================================================
print("\n" + "=" * 80)
print("USE CASE 2: Debugging - Find Problematic Predictions")
print("=" * 80)
print("""
Scenario: Some predictions are wrong. Find instances where unusual
indicator patterns might explain the errors.
""")

start = time.time()

# Detect all outliers
all_outliers = calc.detect_outliers_vectorized()
outlier_counts = np.sum(all_outliers, axis=1)

# Find rows with many outliers (likely problematic)
high_outlier_threshold = np.percentile(outlier_counts, 95)
problematic_rows = np.where(outlier_counts >= high_outlier_threshold)[0]

elapsed = time.time() - start

print(f"\n‚úì Detected outliers across all {n_samples:,} rows in {elapsed:.4f} seconds")
print(f"\nüìä Statistics:")
print(f"  Mean outliers per row: {np.mean(outlier_counts):.2f}")
print(f"  95th percentile: {high_outlier_threshold:.0f}")
print(f"  Rows flagged as problematic: {len(problematic_rows)}")

print(f"\nüîç Most problematic rows (top 5):")
top_problematic = np.argsort(outlier_counts)[-5:][::-1]
for rank, row_idx in enumerate(top_problematic, 1):
    n_outliers = outlier_counts[row_idx]
    print(f"  {rank}. Row {row_idx}: {n_outliers} outlier indicators")

# Get detailed view of most problematic row
worst_row = top_problematic[0]
result = calc.get_row_importance(worst_row, top_k=5)
print(f"\nDetailed analysis of most problematic row ({worst_row}):")
print(f"  Top indicators:")
for name, score in result.top_indicators:
    print(f"    - {name}: {score:.4f}")
print(f"  Number of outliers: {len(result.outlier_indicators)}")

print(f"\nüí° Action: Manually review these {len(problematic_rows)} instances")

# ============================================================================
# USE CASE 3: LF Quality Assessment
# ============================================================================
print("\n" + "=" * 80)
print("USE CASE 3: Labeling Function Quality Assessment")
print("=" * 80)
print("""
Scenario: You have 50 LFs. Which ones are actually useful vs noisy?
""")

start = time.time()

# Get all importance scores
all_importance = calc.get_all_importance_scores(normalize=True)

# For each LF, compute:
# 1. How often it's in top-10
# 2. Average importance when it matters
# 3. Variance (consistency)

top_indices, top_scores = calc.get_top_k_matrix(top_k=10)

lf_quality = []
for lf_idx, lf_name in enumerate(indicator_names):
    # Count appearances in top-10
    in_top_10 = np.sum(top_indices == lf_idx)
    
    # Average absolute importance
    avg_importance = np.mean(np.abs(all_importance[:, lf_idx]))
    
    # Variance (lower = more consistent)
    variance = np.var(all_importance[:, lf_idx])
    
    # Weight (from Snorkel)
    weight = snorkel_weights[lf_idx]
    
    lf_quality.append({
        'name': lf_name,
        'top_10_count': in_top_10,
        'avg_importance': avg_importance,
        'variance': variance,
        'weight': weight
    })

elapsed = time.time() - start

# Sort by top-10 appearances
lf_quality_df = pd.DataFrame(lf_quality).sort_values('top_10_count', ascending=False)

print(f"\n‚úì Assessed all {n_indicators} LFs in {elapsed:.4f} seconds")
print(f"\nTop 10 Most Useful LFs (by top-10 appearances):")
print(lf_quality_df.head(10).to_string(index=False))

print(f"\nBottom 5 Least Useful LFs:")
print(lf_quality_df.tail(5).to_string(index=False))

print(f"\nüí° Action: Consider removing LFs that rarely appear in top-10")

# ============================================================================
# USE CASE 4: Instance Similarity / Clustering
# ============================================================================
print("\n" + "=" * 80)
print("USE CASE 4: Find Similar Instances")
print("=" * 80)
print("""
Scenario: Given a misclassified instance, find other similar instances
that might also be misclassified.
""")

from scipy.spatial.distance import cdist

start = time.time()

# Get all importance scores
all_importance = calc.get_all_importance_scores(normalize=True)

# Pick a target instance (e.g., a known error)
target_idx = 42
target_importance = all_importance[target_idx:target_idx+1]

# Find similar instances using cosine similarity
distances = cdist(target_importance, all_importance, metric='cosine')[0]
similarities = 1 - distances

# Get top-10 most similar (excluding self)
similar_indices = np.argsort(similarities)[:-1][-10:][::-1]  # Exclude last (self)

elapsed = time.time() - start

print(f"\n‚úì Found similar instances in {elapsed:.4f} seconds")
print(f"\nInstances most similar to row {target_idx}:")
print(f"{'Rank':<6} {'Row Index':<12} {'Similarity':<12} {'Outlier Count':<15}")
print("-" * 50)

outlier_counts = np.sum(calc.detect_outliers_vectorized(), axis=1)
for rank, idx in enumerate(similar_indices, 1):
    sim = similarities[idx]
    n_outliers = outlier_counts[idx]
    print(f"{rank:<6} {idx:<12} {sim:>10.4f}  {n_outliers:>13}")

print(f"\nüí° Action: Review these similar instances - likely same failure mode")

# ============================================================================
# USE CASE 5: Feature Engineering Insights
# ============================================================================
print("\n" + "=" * 80)
print("USE CASE 5: Feature Engineering - LF Co-occurrence")
print("=" * 80)
print("""
Scenario: Identify which LF pairs frequently appear together in top-k.
These might be good candidates for interaction features.
""")

start = time.time()

# Get top-5 for all rows
top_indices, top_scores = calc.get_top_k_matrix(top_k=5)

# Count co-occurrences
cooccurrence = Counter()
for i in range(n_samples):
    top_5 = top_indices[i]
    # Count all pairs
    for j in range(5):
        for k in range(j+1, 5):
            pair = tuple(sorted([indicator_names[top_5[j]], indicator_names[top_5[k]]]))
            cooccurrence[pair] += 1

elapsed = time.time() - start

print(f"\n‚úì Analyzed co-occurrences in {elapsed:.4f} seconds")
print(f"\nTop 10 Most Common LF Pairs (in top-5):")
print(f"{'Rank':<6} {'LF Pair':<70} {'Count':<10}")
print("-" * 90)
for rank, (pair, count) in enumerate(cooccurrence.most_common(10), 1):
    lf1, lf2 = pair
    pct = 100 * count / n_samples
    pair_str = f"{lf1[:30]} + {lf2[:30]}"
    print(f"{rank:<6} {pair_str:<70} {count:>5} ({pct:>5.1f}%)")

print(f"\nüí° Insight: Create interaction features for frequently co-occurring LF pairs")

# ============================================================================
# USE CASE 6: Real-time Explanation Service
# ============================================================================
print("\n" + "=" * 80)
print("USE CASE 6: Real-time Explanation Service")
print("=" * 80)
print("""
Scenario: Pre-compute explanations for all instances, store in database
for instant lookup when users request explanations.
""")

start = time.time()

# Pre-compute all top-k and outliers
top_indices, top_scores = calc.get_top_k_matrix(top_k=5)
all_outliers = calc.detect_outliers_vectorized()

# Store in format ready for database
explanations_db = []
for i in range(min(1000, n_samples)):  # Store first 1000 as example
    explanation = {
        'row_id': i,
        'top_lfs': [indicator_names[idx] for idx in top_indices[i]],
        'top_scores': top_scores[i].tolist(),
        'outlier_lfs': [indicator_names[j] for j in np.where(all_outliers[i])[0]],
        'n_outliers': int(np.sum(all_outliers[i]))
    }
    explanations_db.append(explanation)

elapsed = time.time() - start

print(f"\n‚úì Pre-computed explanations for 1000 instances in {elapsed:.4f} seconds")
print(f"  Rate: {1000/elapsed:,.0f} explanations/second")

# Simulate lookup
lookup_id = 42
explanation = explanations_db[lookup_id]

print(f"\nüì¶ Example stored explanation (row {lookup_id}):")
print(f"  Top LFs: {explanation['top_lfs']}")
print(f"  Scores: {[f'{s:.3f}' for s in explanation['top_scores']]}")
print(f"  Outliers: {explanation['n_outliers']}")

print(f"\nüí° Benefit: Instant (<1ms) explanation lookup for user requests")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("KEY TAKEAWAYS")
print("=" * 80)
print("""
‚úÖ Vectorized operations enable production-scale analysis:
   ‚Ä¢ Monitor 10k+ predictions daily in seconds
   ‚Ä¢ Debug problematic instances across entire dataset
   ‚Ä¢ Assess LF quality comprehensively
   ‚Ä¢ Find similar instances for error analysis
   ‚Ä¢ Discover LF interaction patterns
   ‚Ä¢ Pre-compute explanations for instant lookup

üìà Performance at scale:
   ‚Ä¢ 10k rows: ~1 second
   ‚Ä¢ 100k rows: ~10 seconds
   ‚Ä¢ 1M rows: ~100 seconds (with chunking)

üîß Best Practices:
   1. Use vectorized methods for batch operations
   2. Pre-compute and cache when possible
   3. Process large datasets in chunks (10k-100k per chunk)
   4. Store results in database for quick lookup
   5. Use single-row methods only for debugging

üìä Memory usage:
   ‚Ä¢ 10k √ó 50 indicators: ~4 MB
   ‚Ä¢ 100k √ó 100 indicators: ~80 MB
   ‚Ä¢ Efficient even for large datasets
""")

print("=" * 80)
print("‚úÖ Use Cases Demo Complete!")
print("=" * 80)
